## 1. Compare neural learning models
1. classical neural network models
2. biologically inspired neural networks
   update: RNNs done
   todo: CNNs
3. datasets (synthetic/real-world: MNIST digits, time-series)
    update: synthetic + noise done
    todo: check Daniela & Mathias' paper with datasets in ANNEX
6. train models (save training time, model complexity, accuracy, and loss)
7. evaluate: loss, accuracy, runtime
8. compare metrics, runtimes, and model complexities

## 2. LLM interpretability/explainability/reasoning of the neural learning models
1. LLM models (e.g. llamas)
2. Interpretability
3. Explainability
4. Reasoning

## 3. Variation of input representation
1. Text description
2. ...


# Dates of interest:
- ...
- 8 January 2025: Oxford Maths DPhil - application deadline
- 9 January 2025: ICML submission starts
- 17 January 2025: Seminar in Data Science - presentation
- 31 January 2025: ICML submission deadline
- February+: Msc. thesis submission
