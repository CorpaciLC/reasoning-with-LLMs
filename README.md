## 1. Compare neural learning models
1. classical neural network models
   work in progress: RNNs/LSTMs
   todo: CNNs
3. biologically inspired neural networks
   work in progress: LNNs, NPCs
   
5. datasets (synthetic/real-world: MNIST digits, time-series)
    time-series:
      synthetic (sinusoid) + noise
      real-world: har dataset
   images:
      todo: MNIST digits
   
7. train models (save training time, model complexity, accuracy, and loss)
     save: activation function, weight matrices, equations, etc.
9. evaluate: loss, accuracy, runtime
10. compare metrics, runtimes, and model complexities

## 2. LLM interpretability/explainability/reasoning of the neural learning models
1. LLM models (e.g. llamas)
2. Interpretability
3. Explainability
4. Reasoning

## 3. Variation of input representation
1. Text description
2. ...

# Experimental ideas:
...

# Dates of interest:
- ...

### December
- 3rd: ML exam
- 6th: todo: send update to Monika with experimental setup (MVP on TU cluster): expand data on NNs + initial pipeline for llm prompting with 1-parameter variation
- 10th+: LLM experiments / explore edge cases / failure modes 
- 15th: bash scripts for runs on cluster / start experiments
- 20th: initial draft of paper +  intermediate results 
- ...
- 27th: LLM explorations

### January 2025 - Nope
- 3th: Intermediary draft
- **8th**: Oxford Maths DPhil - application **deadline**
- 9th: ICML submission starts
- 17th: Seminar in Data Science - presentation
- 20th: intial final draft
- **24th**: title + abstract + authors **deadline**
- **31th**: ICML submission **deadline**

### February - Nope
Msc. thesis submission

### March
- re-transit into thesis & cover intially discussed experiments
- 1+ overleaf page per day + 1+ commit per day

### April
- 60pages thesis + draft

### May
- 15th May NeurIPS paper deadline

### June
- Graduation
